---
title: "xgb n-1"
author: "Gabriel Macé"
date: "2025-07-02"
output:
  html_document:
    toc: true
    toc_float: true
    theme: united
  pdf_document:
    toc: true
---

```{css, echo=FALSE}
h1 {
    font-family: "Courier New", Courier, monospace; 
    font-size: 36px; 
    font-weight: bold;
    text-decoration: underline;
    text-align: center;
    color: darkblue;
}

h2 {
    text-decoration: underline;
    color: darkred;
}
h3 {
    color: blue;
}

h4 {
    font-style: italic;
    color: lightblue;
}
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
```

```{r}
library(reticulate)
# Dis à R d’utiliser le bon environnement Python
use_python("C:/Users/Lucas/AppData/Local/r-miniconda/envs/bayesenv/python.exe", required = TRUE)
```

```{python}
# Importation des bibliothèques nécessaires
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split,cross_val_score
import pyreadr
```

```{python}
# Chargement des données
observations = pyreadr.read_r("../../data/modelisation/observations_parcelles.RData")["observations"]
observations = pd.DataFrame(observations)
```

### Préparation des données :

```{python}
var_an_pheno = list(observations.columns[13:48]) # ne pas garder la longueur de la période = 365 ou 366
var_an = list(observations.columns[45:85]) # ne pas garder la longueur de la période
var_dormance = list(observations.columns[86:122])
var_deb_to_flo = list(set(observations.columns[123:159]) - set(["sum.heat.days.35.deb_flo", "isv.faible.seq.15.deb_flo", "isv.fai_mod.seq.15.deb_flo", "isv.mod_sev.seq.10.deb_flo", "isv.mod_sev.seq.15.deb_flo"]))
# Enlever sum.heat.days.35.deb_flo car que 0
var_symptomes = list(set(observations.columns[160:196]) - set(["sum.frost.days.0.symptomes"])) # Enlever sum.frost.days.0.symptomes car que 0
var_deb_to_end = list(observations.columns[197:233])

var_tt = var_an_pheno + var_an + var_dormance + var_deb_to_flo + var_symptomes + var_deb_to_end
observations['cepage'] = observations['cepage'].astype('category')
observations['region_viticole'] = observations['region_viticole'].astype('category')
```

```{python}
# Enlever les variables trop corrélées pour ne pas mal interprétées les valeurs de SHAP :

score_var = pd.read_excel("../../data/resultats/r2_rmse_par_variable.xlsx")
# Pour toutes les variables, s'il elle est corrélée à une autre : 
# garder celle qui à la meilleur rmse (glm avec (1|cepage) + (1|region_viticole) + var)
vars = var_tt + list(["RU", "debourrement", "floraison" ])
to_rm = []
for var1 in vars:
    if var1 not in to_rm:
        for var2 in list(set(vars) - set([var1] + to_rm)):
            corr = np.corrcoef(observations[var1], observations[var2])[0, 1]
            if corr > 0.8:
                rmse1 = score_var.loc[score_var["variable"] == var1, "rmse"].values[0]
                rmse2 = score_var.loc[score_var["variable"] == var2, "rmse"].values[0]
                if rmse1 > rmse2:
                    to_rm.append(var1)
                else:
                    to_rm.append(var2)
                break
              
to_keep = list(["cepage", "region_viticole", "age_parcelle_estime"]) + list(set(vars) - set(to_rm))
```

```{python}
climat_n_1 = observations[list(["identifiant_parcelle_analyse","annee"]) + list(set(vars) - set(to_rm))]
climat_n_1.annee = climat_n_1.annee + 1
obs = pd.merge(observations[list(["identifiant_parcelle_analyse","annee", "pourcentage_esca"]) + to_keep], climat_n_1, on = ["identifiant_parcelle_analyse", "annee"], suffixes=('', '_n_1'))
```



```{python}
# Division des données en ensemble d'apprentissage et de test
train_ratio = 0.8

# Trier les observations par année
obs_sort = obs.sort_values(by="annee")

# Initialisation des jeux
train_parts = []
test_parts = []

# Regroupement par identifiant
grouped = obs_sort.groupby("identifiant_parcelle_analyse")

for _, group in grouped:
    n = len(group)
    group = group.reset_index(drop=True)  # pour un index de 0 à n-1
    if n <= 5:
        train_index, test_index = train_test_split(range(0,n), test_size=0.3) # int(np.ceil(n / 2))
    else:
        train_index, test_index = train_test_split(range(0,n), test_size=0.2) # int(np.floor(n * train_ratio))
    
    train_parts.append(group.iloc[train_index])
    test_parts.append(group.iloc[test_index])

# Concaténer les groupes pour obtenir les jeux complets
train = pd.concat(train_parts).reset_index(drop=True)
test = pd.concat(test_parts).reset_index(drop=True)
```

```{python}
train, test = train_test_split(obs, test_size=0.2)
X_train = pd.get_dummies(train.drop(list(['pourcentage_esca', 'identifiant_parcelle_analyse']), axis=1))
X_test = pd.get_dummies(test.drop(list(['pourcentage_esca', 'identifiant_parcelle_analyse']), axis=1))

y_train = train['pourcentage_esca']
y_test = test['pourcentage_esca']
```

## XGBoost

```{python}
from xgboost import XGBRegressor
```

```{python}
best_model = XGBRegressor(learning_rate = 0.1, max_depth = 6, n_estimators = 200, objective = 'count:poisson')
best_model.fit(X_train, y_train)
y_train_pred = best_model.predict(X_train)
y_test_pred = best_model.predict(X_test)

# Report
print("\nTrain RMSE:")
print(np.sqrt(np.mean((y_train - y_train_pred)**2)))
print("\nTrain mean error:")
print(np.mean(np.abs(y_train - y_train_pred)))
print("\nTrain r²:")
print(np.corrcoef(y_train, y_train_pred)[0,1]**2)
print("\nTest RMSE:")
print(np.sqrt(np.mean((y_test - y_test_pred)**2)))
print("\nTest mean error:")
print(np.mean(np.abs(y_test - y_test_pred)))
print("\nTest r²:")
print(np.corrcoef(y_test, y_test_pred)[0,1]**2)
```



```{python}

```

```{python}

```

```{python}
train, test = train_test_split(obs, test_size=0.2)
esca_parcelle = train.groupby('identifiant_parcelle_analyse').agg({'pourcentage_esca': 'median'}).rename(columns={'pourcentage_esca': 'median_esca'})
esca_parcelle.median_esca = round(esca_parcelle.median_esca)
train = pd.merge(train, esca_parcelle, on = "identifiant_parcelle_analyse")
train['median_esca'] = train['median_esca']#.astype('category')
test = pd.merge(test, esca_parcelle, on = "identifiant_parcelle_analyse")
test['median_esca'] = test['median_esca']#.astype('category')

X_train = pd.get_dummies(train.drop(list(['pourcentage_esca', 'identifiant_parcelle_analyse']), axis=1))
X_test = pd.get_dummies(test.drop(list(['pourcentage_esca', 'identifiant_parcelle_analyse']), axis=1))

y_train = train['pourcentage_esca']
y_test = test['pourcentage_esca']
```

```{python}
best_model = XGBRegressor(learning_rate = 0.1, max_depth = 4, n_estimators = 350, objective = 'count:poisson')
# quali : 6, 750
best_model.fit(X_train, y_train)
y_train_pred = best_model.predict(X_train)
y_test_pred = best_model.predict(X_test)

# Report
print("\nTrain RMSE:")
print(np.sqrt(np.mean((y_train - y_train_pred)**2)))
print("\nTrain mean error:")
print(np.mean(np.abs(y_train - y_train_pred)))
print("\nTrain r²:")
print(np.corrcoef(y_train, y_train_pred)[0,1]**2)
print("\nTest RMSE:")
print(np.sqrt(np.mean((y_test - y_test_pred)**2)))
print("\nTest mean error:")
print(np.mean(np.abs(y_test - y_test_pred)))
print("\nTest r²:")
print(np.corrcoef(y_test, y_test_pred)[0,1]**2)
```

```{python}

```


```{python}
climat_n_1 = observations[list(["identifiant_parcelle_analyse","annee", "pourcentage_esca"]) + list(set(vars) - set(to_rm))]
climat_n_1.annee = climat_n_1.annee + 1
obs = pd.merge(observations[list(["identifiant_parcelle_analyse","annee", "pourcentage_esca"]) + to_keep], climat_n_1, on = ["identifiant_parcelle_analyse", "annee"], suffixes=('', '_n_1'))
```



```{python}
# Division des données en ensemble d'apprentissage et de test
train_ratio = 0.8

# Trier les observations par année
obs_sort = obs.sort_values(by="annee")

# Initialisation des jeux
train_parts = []
test_parts = []

# Regroupement par identifiant
grouped = obs_sort.groupby("identifiant_parcelle_analyse")

for _, group in grouped:
    n = len(group)
    group = group.reset_index(drop=True)  # pour un index de 0 à n-1
    if n <= 5:
        train_index, test_index = train_test_split(range(0,n), test_size=0.3) # int(np.ceil(n / 2))
    else:
        train_index, test_index = train_test_split(range(0,n), test_size=0.2) # int(np.floor(n * train_ratio))
    
    train_parts.append(group.iloc[train_index])
    test_parts.append(group.iloc[test_index])

# Concaténer les groupes pour obtenir les jeux complets
train = pd.concat(train_parts).reset_index(drop=True)
test = pd.concat(test_parts).reset_index(drop=True)

X_train = pd.get_dummies(train.drop('pourcentage_esca', axis=1))
X_test = pd.get_dummies(test.drop('pourcentage_esca', axis=1))

y_train = train['pourcentage_esca']
y_test = test['pourcentage_esca']
```

```{python}
train, test = train_test_split(obs, test_size=0.2)

X_train = pd.get_dummies(train.drop(list(['pourcentage_esca', 'identifiant_parcelle_analyse']), axis=1))
X_test = pd.get_dummies(test.drop(list(['pourcentage_esca', 'identifiant_parcelle_analyse']), axis=1))

y_train = train['pourcentage_esca']
y_test = test['pourcentage_esca']
```


## XGBoost

```{python}
from xgboost import XGBRegressor
```

```{python}
best_model = XGBRegressor(learning_rate = 0.1, max_depth = 4, n_estimators = 200, objective = 'count:poisson')
# quali : 4, 200
best_model.fit(X_train, y_train)
y_train_pred = best_model.predict(X_train)
y_test_pred = best_model.predict(X_test)

# Report
print("\nTrain RMSE:")
print(np.sqrt(np.mean((y_train - y_train_pred)**2)))
print("\nTrain mean error:")
print(np.mean(np.abs(y_train - y_train_pred)))
print("\nTrain r²:")
print(np.corrcoef(y_train, y_train_pred)[0,1]**2)
print("\nTest RMSE:")
print(np.sqrt(np.mean((y_test - y_test_pred)**2)))
print("\nTest mean error:")
print(np.mean(np.abs(y_test - y_test_pred)))
print("\nTest r²:")
print(np.corrcoef(y_test, y_test_pred)[0,1]**2)
```


```{python}

```

## Valeur de Shapley

```{python}
obs = observations.copy()

esca_parcelle = obs.groupby('identifiant_parcelle_analyse').agg({'pourcentage_esca': 'median'}).rename(columns={'pourcentage_esca': 'median_esca'})
obs = pd.merge(obs, esca_parcelle, on = "identifiant_parcelle_analyse")
obs['median_esca'] = obs['median_esca'].astype('category')

to_keep = list(["cepage", "median_esca", "region_viticole"]) + list(set(vars) - set(to_rm))

X = pd.get_dummies(obs[to_keep])
y = obs['pourcentage_esca']

best_model = XGBRegressor(learning_rate = 0.1, max_depth = 15, n_estimators = 1000, objective = 'count:poisson')
best_model.fit(X_train, y_train)
```

```{python}

```

```{python}

```

```{python}

```

```{python}

```

```{python}

```


